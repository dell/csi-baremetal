/*
Copyright Â© 2020 Dell Inc. or its subsidiaries. All Rights Reserved.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
*/

package scenarios

import (
	"context"
	"fmt"
	"sync"

	"github.com/google/uuid"
	"github.com/onsi/ginkgo/v2"
	corev1 "k8s.io/api/core/v1"
	storagev1 "k8s.io/api/storage/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	clientset "k8s.io/client-go/kubernetes"
	"k8s.io/kubernetes/test/e2e/framework"
	e2enode "k8s.io/kubernetes/test/e2e/framework/node"
	e2eskipper "k8s.io/kubernetes/test/e2e/framework/skipper"
	storageframework "k8s.io/kubernetes/test/e2e/storage/framework"

	"github.com/dell/csi-baremetal-e2e-tests/e2e/common"
	apiV1 "github.com/dell/csi-baremetal/api/v1"
)

// DefineSchedulerTestSuite defines tests for scheduler extender
func DefineSchedulerTestSuite(driver *baremetalDriver) {
	ginkgo.Context("Baremetal-csi driver scheduling tests", func() {
		schedulingTest(driver)
	})
}

var (
	storageClassAny    = apiV1.StorageClassAny
	storageClassHDD    = apiV1.StorageClassHDD
	storageClassSSD    = apiV1.StorageClassSSD
	storageClassNVMe   = apiV1.StorageClassNVMe
	storageClassHDDLVG = apiV1.StorageClassHDDLVG
	storageClassSSDLVG = apiV1.StorageClassSSDLVG

	driveTypeHDD  = apiV1.DriveTypeHDD
	driveTypeSSD  = apiV1.DriveTypeSSD
	driveTypeNVMe = apiV1.DriveTypeNVMe
)

// schedulingTest test custom extender for scheduler
func schedulingTest(driver *baremetalDriver) {
	ginkgo.BeforeEach(skipIfNotAllTests)

	var (
		testPODs       []*corev1.Pod
		testPVCs       []*corev1.PersistentVolumeClaim
		updateM        sync.Mutex
		ctx            context.Context
		ns             string
		f              = framework.NewDefaultFramework("scheduling-test")
		availableSC    = []string{storageClassAny, storageClassHDD, storageClassSSD,
			storageClassNVMe, storageClassHDDLVG, storageClassSSDLVG}
		storageClasses = make(map[string]*storagev1.StorageClass)
	)

	init := func(lmConf *common.LoopBackManagerConfig) {
		var (
			perTestConf *storageframework.PerTestConfig
			err         error
		)
		ns = f.Namespace.Name
		ctx = context.Background()

		testPODs, testPVCs = nil, nil
		storageClasses = make(map[string]*storagev1.StorageClass)

		if lmConf != nil {
			lmConfigMap, err := common.BuildLoopBackManagerConfigMap(ns, cmName, *lmConf)
			framework.ExpectNoError(err)
			_, err = f.ClientSet.CoreV1().ConfigMaps(ns).Create(ctx, lmConfigMap, metav1.CreateOptions{})
		}

		perTestConf = PrepareCSI(ctx, driver, f, false)

		for _, scName := range availableSC {
			sc := driver.GetStorageClassWithStorageType(perTestConf, scName)
			sc, err = f.ClientSet.StorageV1().StorageClasses().Create(ctx, sc, metav1.CreateOptions{})
			framework.ExpectNoError(err)
			storageClasses[scName] = sc
		}
	}

	cleanup := func() {
		framework.Logf("Starting cleanup for test SchedulingTests")
		common.CleanupAfterCustomTest(ctx, f, func(){}, testPODs, testPVCs)

		err := f.ClientSet.CoreV1().ConfigMaps(ns).Delete(ctx, cmName, metav1.DeleteOptions{})
		if err != nil {
			framework.Logf("Configmap %s deletion failed: %v", cmName, err)
		}
	}

	createTestPod := func(podSCList []string) (*corev1.Pod, []*corev1.PersistentVolumeClaim) {
		var podPVCs []*corev1.PersistentVolumeClaim
		for _, scKey := range podSCList {
			sc := storageClasses[scKey]
			pvc, err := f.ClientSet.CoreV1().PersistentVolumeClaims(ns).Create(
				ctx,
				constructPVC(ns, driver.GetClaimSize(), sc.Name, pvcName+"-"+uuid.New().String()),
				metav1.CreateOptions{})
			framework.ExpectNoError(err)
			podPVCs = append(podPVCs, pvc)
		}
		pod := startAndWaitForPodWithPVCRunning(ctx, f, ns, podPVCs)
		updateM.Lock()
		testPODs = append(testPODs, pod)
		testPVCs = append(testPVCs, podPVCs...)
		updateM.Unlock()
		return pod, podPVCs
	}

	createTestPods := func(testPodsCount int, testPodsDisksPerPod int) {
		wg := sync.WaitGroup{}
		var podSCList []string
		for i := 0; i < testPodsDisksPerPod; i++ {
			podSCList = append(podSCList, "ANY")
		}
		for i := 0; i < testPodsCount; i++ {
			wg.Add(1)
			go func() {
				defer ginkgo.GinkgoRecover()
				defer wg.Done()
				_, _ = createTestPod(podSCList)
			}()
		}
		wg.Wait()
	}

	ginkgo.It("One node has all capacity", func(ctx context.Context) {
		testPodsCount := 3
		testPodsDisksPerPod := 2

		nodes := getSchedulableNodesNamesOrSkipTest(ctx, f.ClientSet, 2)
		nodeWithDisksID := nodes[0]
		nodeWithDisksDriveCount := testPodsCount * testPodsDisksPerPod
		defaultDriveCount := 0
		lmConfig := &common.LoopBackManagerConfig{
			DefaultDriveCount: &defaultDriveCount,
			Nodes: []common.LoopBackManagerConfigNode{
				{
					NodeID:     &nodeWithDisksID,
					DriveCount: &nodeWithDisksDriveCount},
			},
		}
		init(lmConfig)
		defer cleanup()
		createTestPods(testPodsCount, testPodsDisksPerPod)
	})

	ginkgo.It("PODs should distribute across nodes", func(ctx context.Context) {
		testPodsCount := 3
		testPodsDisksPerPod := 3
		nodes := getSchedulableNodesNamesOrSkipTest(ctx, f.ClientSet, testPodsCount)

		defaultDriveCount := 0
		var lmNodes []common.LoopBackManagerConfigNode
		for _, n := range nodes[:testPodsCount] {
			nodeName := n
			nodeDriveCount := testPodsDisksPerPod
			lmNodes = append(lmNodes,
				common.LoopBackManagerConfigNode{
					NodeID:     &nodeName,
					DriveCount: &nodeDriveCount})
		}
		lmConfig := &common.LoopBackManagerConfig{
			DefaultDriveCount: &defaultDriveCount,
			Nodes:             lmNodes}
		init(lmConfig)
		defer cleanup()
		createTestPods(testPodsCount, testPodsDisksPerPod)
	})

	ginkgo.It("Scheduler should respect SC", func(ctx context.Context) {
		nodes := getSchedulableNodesNamesOrSkipTest(ctx, f.ClientSet, 3)

		node1, node2, node3 := nodes[0], nodes[1], nodes[2]

		defaultDriveCount := 0
		lmConfig := &common.LoopBackManagerConfig{
			DefaultDriveCount: &defaultDriveCount,
			Nodes: []common.LoopBackManagerConfigNode{
				*buildLMDrivesConfig(node1, []common.LoopBackManagerConfigDevice{
					{DriveType: &driveTypeHDD}, {DriveType: &driveTypeSSD}}),
				*buildLMDrivesConfig(node2, []common.LoopBackManagerConfigDevice{
					{DriveType: &driveTypeHDD}, {DriveType: &driveTypeNVMe}, {DriveType: &driveTypeHDD}}),
				*buildLMDrivesConfig(node3, []common.LoopBackManagerConfigDevice{
					{DriveType: &driveTypeHDD}, {DriveType: &driveTypeHDD}}),
			}}
		init(lmConfig)
		defer cleanup()

		createTestPod([]string{storageClassHDD, storageClassSSD})
		createTestPod([]string{storageClassHDD, storageClassNVMe})
		createTestPod([]string{storageClassHDD, storageClassHDD})
		createTestPod([]string{storageClassAny})
	})

	ginkgo.It("2 LVM PV on one drive", func(ctx context.Context) {
		nodes := getSchedulableNodesNamesOrSkipTest(ctx, f.ClientSet, 2)
		defaultDriveCount := 0
		node1, node2 := nodes[0], nodes[1]
		driveSize := "250Mi"
		lmConfig := &common.LoopBackManagerConfig{
			DefaultDriveCount: &defaultDriveCount,
			Nodes: []common.LoopBackManagerConfigNode{
				*buildLMDrivesConfig(node1, []common.LoopBackManagerConfigDevice{
					{DriveType: &driveTypeHDD, Size: &driveSize}}),
				*buildLMDrivesConfig(node2, []common.LoopBackManagerConfigDevice{
					{DriveType: &driveTypeSSD, Size: &driveSize}}),
			}}
		init(lmConfig)
		defer cleanup()

		createTestPod([]string{storageClassHDDLVG, storageClassHDDLVG})
		createTestPod([]string{storageClassSSDLVG, storageClassSSDLVG})
	})

	ginkgo.It("PODs should distribute across nodes with sequential deploy", func(ctx context.Context) {
		// TODO: change result verification https://github.com/dell/csi-baremetal/issues/153
		ginkgo.Skip("We shouldn't check prioritize work based on kube-scheduler decision, ISSUE-153")
		testPodsDisksPerPod := 1
		nodes := getSchedulableNodesNamesOrSkipTest(ctx, f.ClientSet, 0)
		testPodsCount := len(nodes)
		defaultDriveCount := 0

		var lmNodes []common.LoopBackManagerConfigNode
		for _, n := range nodes {
			nodeName := n
			// we need to make sure we have capacity for all pod in a single node
			nodeDriveCount := testPodsCount * testPodsDisksPerPod
			lmNodes = append(lmNodes,
				common.LoopBackManagerConfigNode{
					NodeID:     &nodeName,
					DriveCount: &nodeDriveCount})
		}
		lmConfig := &common.LoopBackManagerConfig{
			DefaultDriveCount: &defaultDriveCount,
			Nodes:             lmNodes}
		init(lmConfig)
		defer cleanup()

		// create pods sequential
		for p := 0; p < testPodsCount; p++ {
			createTestPod([]string{"ANY"})
		}
		volumes := getVolumesByNodes(f)
		framework.Logf("volumes by nodes %v", volumes)

		err := hostsNeedToHaveEqualNumberVolumes(volumes, 1)
		framework.ExpectNoError(err)
	})
}

func getVolumesByNodes(f *framework.Framework) map[string][]string {
	volumesUnstructuredList, err := f.DynamicClient.Resource(common.VolumeGVR).List(context.TODO(), metav1.ListOptions{})
	framework.ExpectNoError(err)
	volumes := make(map[string][]string)
	for _, targetVolume := range volumesUnstructuredList.Items {
		nodeUIDOfVolume, _, err := unstructured.NestedString(targetVolume.Object, "spec", "NodeId")
		framework.ExpectNoError(err)
		nodeNameOfVolume, err := findNodeNameByUID(f, nodeUIDOfVolume)
		framework.ExpectNoError(err)
		volId, _, err := unstructured.NestedString(targetVolume.Object, "spec", "Id")
		if _, ok := volumes[nodeNameOfVolume]; ok {
			volumes[nodeNameOfVolume] = append(volumes[nodeNameOfVolume], volId)
			continue
		}
		volumes[nodeNameOfVolume] = []string{volId}
	}
	return volumes
}

func hostsNeedToHaveEqualNumberVolumes(volumes map[string][]string, count int) error {
	problemHosts := make([]string, 0)
	for host, volumes := range volumes {
		if count != len(volumes) {
			problemHosts = append(problemHosts, host)
		}
	}
	if len(problemHosts) == 0 {
		return nil
	}
	return fmt.Errorf("hosts %v don't have expected number of volumes", problemHosts)
}

func buildLMDrivesConfig(node string, drives []common.LoopBackManagerConfigDevice) *common.LoopBackManagerConfigNode {
	drivesCount := len(drives)
	return &common.LoopBackManagerConfigNode{
		NodeID:     &node,
		DriveCount: &drivesCount,
		Drives:     drives,
	}
}

func getSchedulableNodesNamesOrSkipTest(ctx context.Context, client clientset.Interface, minNodeCount int) []string {
	result, err := getSchedulableNodesNames(ctx, client, minNodeCount)
	if err != nil {
		e2eskipper.Skipf("test's prerequisites not met: %s", err.Error())
	}
	return result
}

// getSchedulableNodesNames returns list of schedulable nodes
// will return error if schedulable nodes count < minNodeCount
// minNodeCount == 0 mean no limit
func getSchedulableNodesNames(ctx context.Context, client clientset.Interface, minNodeCount int) ([]string, error) {
	nodes, err := e2enode.GetReadySchedulableNodes(ctx, client)
	framework.ExpectNoError(err)
	var nodeNames []string
	for _, item := range nodes.Items {
		nodeNames = append(nodeNames, item.Name)
	}
	if minNodeCount > 0 && minNodeCount > len(nodeNames) {
		return nil, fmt.Errorf("not enough schedulabel nodes, required: %d, found: %d",
			minNodeCount, len(nodeNames))
	}
	return nodeNames, nil
}
